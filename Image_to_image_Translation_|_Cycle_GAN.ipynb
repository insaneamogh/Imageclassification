{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Image to image Translation  | Cycle GAN",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insaneamogh/Imageclassification/blob/main/Image_to_image_Translation_%7C_Cycle_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'horse2zebra-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F926321%2F1567596%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240331%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240331T145858Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6fd704768f3ee9bc70b3fc0abce2afdb135e0a391973e828b2920d009161b069fcb431e97515c275c3bdecccda85e6ce3201820fd5cc2ae7d6ace31a7a37349c6cf0d390982b6af4592e4125298290aeef9e36cb07e8360d0200e6700eff3e234b3af8370572900184686d7ded9c2503eefce832a891a1978eef1f04e75890ad6c421493e625f57b6a4c957f3bd720ba46ce125d1886f9eed7842135e94692fc62f00b91210efb5f208560dad11ddbbb259b3e93124992906adb60728d6656a1565346cbe01e7a087508791969d76b5755d3421c955094a30f8c0406c047bced05e9a35389bcff24999225b099393f9b49c5070018483e23d6723986c4adf9a9,imagetoimage-translation-cyclegan:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2486025%2F4217474%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240331%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240331T145858Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D260505563579d519a13bf08ac7988ccce6fdd51f09788038951c0fad04be6e2cf68362f7f8851662dfbae9ca109b10e43663ccfe9320fd5dd712e0dfea52365815f910edfda8cf3ff20ffc58d312513f072419d931e31782f92ca4da625875f5ee920643e719a6918633941dde0323a4fdf3ee5c5c55ad764019130911925163086d98a8c191c8f0b5466284721a0184b37b6b397ea7a94240a3af5f54047d42993081ca245aad4255554b27834fa14cb0dd6247b4edb6394d1de4845cc80f9e77849c32738f260e1ea113ae7cfc9baccdd4eaa9459ad89562c4a8f8fcee45b2fda753c685e8932b1e51279623f7d0130d2b9af5d2eefc3726d9c4c4844e5cbd'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "WALqLb89BWvO"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output as clear\n",
        "!pip install tensorflow-addons\n",
        "clear()"
      ],
      "metadata": {
        "id": "zD5DuyT-ptSx",
        "execution": {
          "iopub.status.busy": "2022-09-18T02:16:44.656935Z",
          "iopub.execute_input": "2022-09-18T02:16:44.65747Z",
          "iopub.status.idle": "2022-09-18T02:16:57.449715Z",
          "shell.execute_reply.started": "2022-09-18T02:16:44.65735Z",
          "shell.execute_reply": "2022-09-18T02:16:57.448626Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common\n",
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from random import random\n",
        "\n",
        "# Data\n",
        "import tensorflow.image as tfi\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Model Layers\n",
        "from keras.layers import ReLU\n",
        "from keras.layers import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Activation\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import ZeroPadding2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "\n",
        "# Model Functions\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.initializers import RandomNormal\n",
        "\n",
        "# Optimizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Loss\n",
        "from keras.losses import BinaryCrossentropy\n",
        "\n",
        "# Model Viz\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "jtA0aiMnKFSn",
        "execution": {
          "iopub.status.busy": "2022-09-18T02:16:57.452358Z",
          "iopub.execute_input": "2022-09-18T02:16:57.452735Z",
          "iopub.status.idle": "2022-09-18T02:17:04.631393Z",
          "shell.execute_reply.started": "2022-09-18T02:16:57.452691Z",
          "shell.execute_reply": "2022-09-18T02:17:04.630139Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image, title=None):\n",
        "    '''\n",
        "    The function takes in an image and plots it using Matplotlib.\n",
        "    '''\n",
        "    plt.imshow(image)\n",
        "    plt.title(title)\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "LBg99pULK9zP",
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:04.633097Z",
          "iopub.execute_input": "2022-09-18T02:17:04.633851Z",
          "iopub.status.idle": "2022-09-18T02:17:04.642335Z",
          "shell.execute_reply.started": "2022-09-18T02:17:04.633815Z",
          "shell.execute_reply": "2022-09-18T02:17:04.641188Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_horse_path = '../input/horse2zebra-dataset/trainA'\n",
        "root_zebra_path = '../input/horse2zebra-dataset/trainB'\n",
        "horse_paths = sorted(glob(root_horse_path + '/*.jpg'))[:1000]\n",
        "zebra_paths = sorted(glob(root_zebra_path + '/*.jpg'))[:1000]"
      ],
      "metadata": {
        "id": "uxQZCbG2MNQ6",
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:04.646559Z",
          "iopub.execute_input": "2022-09-18T02:17:04.647603Z",
          "iopub.status.idle": "2022-09-18T02:17:05.387038Z",
          "shell.execute_reply.started": "2022-09-18T02:17:04.647543Z",
          "shell.execute_reply": "2022-09-18T02:17:05.38571Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SIZE = 256\n",
        "horse_images, zebra_images = np.zeros(shape=(len(horse_paths),SIZE,SIZE,3)), np.zeros(shape=(len(horse_paths),SIZE,SIZE,3))\n",
        "for i,(horse_path, zebra_path) in tqdm(enumerate(zip(horse_paths, zebra_paths)), desc='Loading'):\n",
        "\n",
        "    horse_image = img_to_array(load_img(horse_path))\n",
        "    horse_image = tfi.resize(tf.cast(horse_image, tf.float32)/255., (SIZE, SIZE))\n",
        "\n",
        "    zebra_image = img_to_array(load_img(zebra_path))\n",
        "    zebra_image = tfi.resize(tf.cast(zebra_image,tf.float32)/255., (SIZE, SIZE))\n",
        "\n",
        "    # as the data is unpaired so we don't have to worry about, positioning the images.\n",
        "\n",
        "    horse_images[i] = horse_image\n",
        "    zebra_images[i] = zebra_image"
      ],
      "metadata": {
        "id": "j2iUmQekMm5Y",
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:05.388835Z",
          "iopub.execute_input": "2022-09-18T02:17:05.389223Z",
          "iopub.status.idle": "2022-09-18T02:17:23.436621Z",
          "shell.execute_reply.started": "2022-09-18T02:17:05.38919Z",
          "shell.execute_reply": "2022-09-18T02:17:23.434335Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [horse_images, zebra_images]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.438242Z",
          "iopub.execute_input": "2022-09-18T02:17:23.438738Z",
          "iopub.status.idle": "2022-09-18T02:17:23.447712Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.438696Z",
          "shell.execute_reply": "2022-09-18T02:17:23.445921Z"
        },
        "trusted": true,
        "id": "nWnK6d-gBWvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing\n",
        "for i in range(10):\n",
        "    id = np.random.randint(len(horse_images))\n",
        "    horse, zebra = horse_images[id], zebra_images[id]\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    show_image(horse)\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    show_image(zebra)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9Nlv7TN_OAH0",
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2022-09-18T02:18:55.968238Z",
          "iopub.execute_input": "2022-09-18T02:18:55.968601Z",
          "iopub.status.idle": "2022-09-18T02:18:58.926122Z",
          "shell.execute_reply.started": "2022-09-18T02:18:55.968571Z",
          "shell.execute_reply": "2022-09-18T02:18:58.925133Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResidualBlock(filters, layer, index):\n",
        "#     init = RandomNormal(stddev=0.02)\n",
        "\n",
        "    x = Conv2D(filters, kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal', use_bias=False, name=\"Block_{}_Conv1\".format(index))(layer)\n",
        "    x = InstanceNormalization(axis=-1, name=\"Block_{}_Normalization1\".format(index))(x)\n",
        "    x = ReLU(name=\"Block_{}_ReLU\".format(index))(x)\n",
        "\n",
        "    x = Conv2D(filters, kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal', use_bias=False, name=\"Block_{}_Conv2\".format(index))(x)\n",
        "    x = InstanceNormalization(axis=-1, name=\"Block_{}_Normalization2\".format(index))(x)\n",
        "\n",
        "    x = concatenate([x, layer], name=\"Block_{}_Merge\".format(index))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.464481Z",
          "iopub.execute_input": "2022-09-18T02:17:23.46481Z",
          "iopub.status.idle": "2022-09-18T02:17:23.479119Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.464782Z",
          "shell.execute_reply": "2022-09-18T02:17:23.477664Z"
        },
        "trusted": true,
        "id": "kfY7G5I8BWvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:9b13c8e0-ce43-4527-878a-be0137e9758c.png)"
      ],
      "metadata": {
        "id": "8THaVcPgBWvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample(filters, layer, size=3, strides=2, activation=None, index=None, norm=True):\n",
        "    x = Conv2D(filters, kernel_size=size, strides=strides, padding='same', kernel_initializer='he_normal', use_bias=False, name=\"Encoder_{}_Conv\".format(index))(layer)\n",
        "    if norm:\n",
        "        x = InstanceNormalization(axis=-1, name=\"Encoder_{}_Normalization\".format(index))(x)\n",
        "    if activation is not None:\n",
        "        x = Activation(activation, name=\"Encoder_{}_Activation\".format(index))(x)\n",
        "    else:\n",
        "        x = LeakyReLU( name=\"Encoder_{}_LeakyReLU\".format(index))(x)\n",
        "    return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.480893Z",
          "iopub.execute_input": "2022-09-18T02:17:23.48125Z",
          "iopub.status.idle": "2022-09-18T02:17:23.488221Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.481221Z",
          "shell.execute_reply": "2022-09-18T02:17:23.487272Z"
        },
        "trusted": true,
        "id": "D9q2dkVbBWvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsample(filters, layer, size=3, strides=2, index=None):\n",
        "    x = Conv2DTranspose(filters, kernel_size=size, strides=strides, padding='same', kernel_initializer='he_normal', use_bias=False, name=\"Decoder_{}_ConvT\".format(index))(layer)\n",
        "    x = InstanceNormalization(axis=-1, name=\"Decoder_{}_Normalization\".format(index))(x)\n",
        "    x = ReLU( name=\"Encoder_{}_ReLU\".format(index))(x)\n",
        "    return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.491524Z",
          "iopub.execute_input": "2022-09-18T02:17:23.492374Z",
          "iopub.status.idle": "2022-09-18T02:17:23.505975Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.492333Z",
          "shell.execute_reply": "2022-09-18T02:17:23.504472Z"
        },
        "trusted": true,
        "id": "-1vq0KGjBWvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Upsample** will perform the **exact opposite** and will **increase the image size by 2**."
      ],
      "metadata": {
        "id": "PW9sJWXwBWvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Generator(n_resnet=9, name=\"Generator\"):\n",
        "\n",
        "    inp_image = Input(shape=(SIZE, SIZE, 3), name=\"InputImage\")         # 256 x 256 x3\n",
        "\n",
        "    x = downsample(64, inp_image, size=7, strides=1, index=1)           # 256 x 256 x 64\n",
        "    x = downsample(128, x, index=2)                                     # 128 x 128 x 128\n",
        "    x = downsample(256, x, index=3)                                     # 64 x 64 x 256\n",
        "\n",
        "    for i in range(n_resnet):\n",
        "        x = ResidualBlock(256, x, index=i+4)                             # (64 x 64 x 256) x n_resnet\n",
        "\n",
        "    x = upsample(128, x, index=13)                                       # 128 x 128 x 128\n",
        "    x = upsample(64, x, index=14)                                        # 256 x 256 x 64\n",
        "    x = downsample(3, x, size=7, strides=1, activation='tanh', index=15) # 256 x 256 x 3\n",
        "\n",
        "    model = Model(\n",
        "        inputs=inp_image,\n",
        "        outputs=x,\n",
        "        name=name\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.507648Z",
          "iopub.execute_input": "2022-09-18T02:17:23.508028Z",
          "iopub.status.idle": "2022-09-18T02:17:23.51874Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.507994Z",
          "shell.execute_reply": "2022-09-18T02:17:23.517623Z"
        },
        "trusted": true,
        "id": "TYojLVR1BWvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Discriminator(name='Discriminator'):\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    src_img = Input(shape=(SIZE, SIZE, 3), name=\"InputImage\")     # 256 x 256 x 3\n",
        "    x = downsample(64, src_img, size=4, strides=2, index=1, norm=False) # 128 x 128 x 64\n",
        "    x = downsample(128, x, size=4, strides=2, index=2)            # 64 x 64 x 128\n",
        "    x = downsample(256, x, size=4, strides=2, index=3)            # 32 x 32 x 256\n",
        "    x = downsample(512, x, size=4, strides=2, index=4)            # 16 x 16 x 512\n",
        "    x = downsample(512, x, size=4, strides=2, index=5)            # 8 x 8 x 512  # we can try out a different architecture with zero padding layer.\n",
        "    patch_out = Conv2D(1, kernel_size=4, padding='same', kernel_initializer=init, use_bias=False)(x) # 8 x 8 x 1\n",
        "\n",
        "    model = Model(\n",
        "        inputs=src_img,\n",
        "        outputs=patch_out,\n",
        "        name=name\n",
        "    )\n",
        "    model.compile(\n",
        "        loss='mse',\n",
        "        optimizer=Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "        loss_weights=[0.5]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.519912Z",
          "iopub.execute_input": "2022-09-18T02:17:23.520739Z",
          "iopub.status.idle": "2022-09-18T02:17:23.532098Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.520698Z",
          "shell.execute_reply": "2022-09-18T02:17:23.531206Z"
        },
        "trusted": true,
        "id": "0yY1VssCBWvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CombineModel(g_model1, g_model2, d_model, name):\n",
        "    # train the Generator\n",
        "    g_model1.trainable = True\n",
        "\n",
        "    # Stop the Discriminator and 2nd Generator\n",
        "    d_model.trainable = False\n",
        "    g_model2.trainable = False\n",
        "\n",
        "    # Adversarial Loss\n",
        "    input_gen = Input(shape=(SIZE, SIZE, 3))\n",
        "    gen_1_out = g_model1(input_gen)\n",
        "    dis_out = d_model(gen_1_out)\n",
        "\n",
        "    # Identity Loss\n",
        "    input_id = Input(shape=(SIZE, SIZE, 3))\n",
        "    output_id = g_model1(input_id)\n",
        "\n",
        "    # Cycle Loss - Forward\n",
        "    output_f = g_model2(gen_1_out)\n",
        "\n",
        "    # Cycle Loss - Backward\n",
        "    gen_2_out = g_model2(input_id)\n",
        "    output_b = g_model1(gen_2_out)\n",
        "\n",
        "    # Final Model\n",
        "    model = Model(\n",
        "        inputs=[input_gen, input_id],\n",
        "        outputs=[dis_out, output_id, output_f, output_b],\n",
        "        name=name\n",
        "    )\n",
        "    model.compile(\n",
        "        loss=['mse', 'mae', 'mae', 'mae'],\n",
        "        loss_weights=[1,5,10,10],\n",
        "        optimizer= Adam(learning_rate=2e-4, beta_1=0.5)\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.533275Z",
          "iopub.execute_input": "2022-09-18T02:17:23.534433Z",
          "iopub.status.idle": "2022-09-18T02:17:23.549958Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.534384Z",
          "shell.execute_reply": "2022-09-18T02:17:23.54887Z"
        },
        "trusted": true,
        "id": "NxZka89eBWvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_real_samples(n_samples, dataset):\n",
        "    ix = np.random.randint(0,dataset.shape[0], n_samples)\n",
        "    X = dataset[ix]\n",
        "    y = np.ones(shape=(n_samples, 8, 8, 1))\n",
        "    return X, y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.551234Z",
          "iopub.execute_input": "2022-09-18T02:17:23.55227Z",
          "iopub.status.idle": "2022-09-18T02:17:23.561447Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.552179Z",
          "shell.execute_reply": "2022-09-18T02:17:23.560416Z"
        },
        "trusted": true,
        "id": "SKnewOYMBWvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_samples(g_model, dataset):\n",
        "    X = g_model.predict(dataset)\n",
        "    y = np.zeros(shape=(len(dataset), 8, 8, 1))\n",
        "    return X, y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.564531Z",
          "iopub.execute_input": "2022-09-18T02:17:23.565678Z",
          "iopub.status.idle": "2022-09-18T02:17:23.578011Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.565631Z",
          "shell.execute_reply": "2022-09-18T02:17:23.577094Z"
        },
        "trusted": true,
        "id": "8IcHxAp6BWvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All these functions are self-explanatory."
      ],
      "metadata": {
        "id": "V_Hz_MkmBWvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_image_pool(pool, images, max_size=50):\n",
        "    selected = list()\n",
        "    for image in images:\n",
        "        if len(pool) < max_size:\n",
        "            pool.append(image)\n",
        "            selected.append(image)\n",
        "        elif random() < 0.5:\n",
        "            selected.append(image)\n",
        "        else:\n",
        "            ix = np.random.randint(0,len(pool))\n",
        "            selected.append(pool[ix])\n",
        "            pool[ix] = image\n",
        "    return np.asarray(selected)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.579694Z",
          "iopub.execute_input": "2022-09-18T02:17:23.580427Z",
          "iopub.status.idle": "2022-09-18T02:17:23.590512Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.580382Z",
          "shell.execute_reply": "2022-09-18T02:17:23.589285Z"
        },
        "trusted": true,
        "id": "nhL5uw5EBWvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_preds(g_AB, g_BA,n_images=1):\n",
        "    for i in range(n_images):\n",
        "\n",
        "        id = np.random.randint(len(horse_images))\n",
        "        horse, zebra = horse_images[id], zebra_images[id]\n",
        "        horse_pred, zebra_pred = g_BA.predict(tf.expand_dims(zebra,axis=0))[0], g_AB.predict(tf.expand_dims(horse,axis=0))[0]\n",
        "\n",
        "        plt.figure(figsize=(10,8))\n",
        "\n",
        "        plt.subplot(1,4,1)\n",
        "        show_image(horse, title='Original Horse')\n",
        "\n",
        "        plt.subplot(1,4,2)\n",
        "        show_image(zebra_pred, title='Generated Zebra')\n",
        "\n",
        "        plt.subplot(1,4,3)\n",
        "        show_image(zebra, title='Original Zebra')\n",
        "\n",
        "        plt.subplot(1,4,4)\n",
        "        show_image(horse_pred, title='Genrated Horse')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.592195Z",
          "iopub.execute_input": "2022-09-18T02:17:23.592973Z",
          "iopub.status.idle": "2022-09-18T02:17:23.609226Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.592928Z",
          "shell.execute_reply": "2022-09-18T02:17:23.608008Z"
        },
        "trusted": true,
        "id": "bp2i0WSLBWvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(d_model_A, d_model_B, gen_AB, gen_BA, c_AB, c_BA, epochs=10, chunk=5):\n",
        "\n",
        "    n_epochs, n_batch = epochs, 1\n",
        "\n",
        "    trainA, trainB = dataset\n",
        "    poolA, poolB = list(), list()\n",
        "\n",
        "    # in simple words, we are going through the whole data.\n",
        "    bat_per_epoch = int(len(trainA)/n_batch)\n",
        "    n_steps = bat_per_epoch\n",
        "\n",
        "    for j in tqdm(range(1,epochs+1), desc=\"Epochs\"):\n",
        "        for i in range(n_steps):\n",
        "\n",
        "            # Let's get some real data in hand.\n",
        "            X_realA, y_realA = generate_real_samples(n_batch, trainA)\n",
        "            X_realB, y_realB = generate_real_samples(n_batch, trainB)\n",
        "\n",
        "            # use our generators to generate some fake data.\n",
        "            X_fakeA, y_fakeA = generate_fake_samples(gen_BA, X_realB)\n",
        "            X_fakeB, y_fakeB = generate_fake_samples(gen_AB, X_realA)\n",
        "\n",
        "            # create a pool of images. You can also understand it like a replay buffer.\n",
        "            X_fakeA = update_image_pool(poolA, X_fakeA)\n",
        "            X_fakeB = update_image_pool(poolA, X_fakeB)\n",
        "\n",
        "\n",
        "            # Let's finally train the gen 2 and get the losses.\n",
        "            gen_loss2, _, _, _, _ = c_BA.train_on_batch(\n",
        "                [X_realB, X_realA],\n",
        "                [y_realB, X_realA, X_realB, X_realA]\n",
        "            )\n",
        "\n",
        "            # It's time for our discriminator to win our generator.\n",
        "            dA_loss_1 = d_model_A.train_on_batch(X_realA, y_realA)\n",
        "            dA_loss_2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n",
        "\n",
        "            # one cycle is completed, let's move to second cycle.\n",
        "            gen_loss1, _, _, _, _ = c_AB.train_on_batch(\n",
        "                [X_realA, X_realB],\n",
        "                [y_realA, X_realB, X_realA, X_realB]\n",
        "            )\n",
        "\n",
        "            # again, let's give some power to our discriminators.\n",
        "            dB_loss_1 = d_model_B.train_on_batch(X_realB, y_realB)\n",
        "            dB_loss_2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n",
        "\n",
        "        if (j%chunk)==0:\n",
        "            show_preds(gen_AB, gen_BA, n_images=1)\n",
        "            gen_AB.save(\"GeneratorHtoZ.h5\")\n",
        "            gen_BA.save(\"GeneratorZtoH.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.6112Z",
          "iopub.execute_input": "2022-09-18T02:17:23.611668Z",
          "iopub.status.idle": "2022-09-18T02:17:23.62903Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.611623Z",
          "shell.execute_reply": "2022-09-18T02:17:23.627997Z"
        },
        "trusted": true,
        "id": "Ca2QjiJ_BWva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "_SReeQomBWva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's create our generators.\n",
        "g_AB = Generator(name=\"GeneratorAB\")\n",
        "g_BA = Generator(name=\"GeneratorBA\")\n",
        "\n",
        "# here are the respective discriminators.\n",
        "d_A = Discriminator(name=\"DiscriminatorA\")\n",
        "d_B = Discriminator(name=\"DiscriminatorB\")\n",
        "\n",
        "# finally, let's combine all of them.\n",
        "c_AB = CombineModel(g_AB, g_BA, d_B, name=\"GanAB\")\n",
        "c_BA = CombineModel(g_BA, g_AB, d_A, name=\"GanBA\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:23.6309Z",
          "iopub.execute_input": "2022-09-18T02:17:23.631685Z",
          "iopub.status.idle": "2022-09-18T02:17:34.332534Z",
          "shell.execute_reply.started": "2022-09-18T02:17:23.63164Z",
          "shell.execute_reply": "2022-09-18T02:17:34.331516Z"
        },
        "trusted": true,
        "id": "K0YiR9AOBWva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the name **A&B** so that the model can be **generalized to any data set**."
      ],
      "metadata": {
        "id": "yuvsOPFMBWva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # it's time to give them the superior knowledge.\n",
        "# train(d_A, d_B, g_AB, g_BA, c_AB, c_BA, epochs=100, chunk=5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:34.333725Z",
          "iopub.execute_input": "2022-09-18T02:17:34.334688Z",
          "iopub.status.idle": "2022-09-18T02:17:34.339093Z",
          "shell.execute_reply.started": "2022-09-18T02:17:34.334654Z",
          "shell.execute_reply": "2022-09-18T02:17:34.337915Z"
        },
        "trusted": true,
        "id": "LSL3ZmMjBWva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# g_AB.save(\"GeneratorHtoZ.h5\")\n",
        "# g_BA.save(\"GeneratorZtoH.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:34.340788Z",
          "iopub.execute_input": "2022-09-18T02:17:34.341919Z",
          "iopub.status.idle": "2022-09-18T02:17:34.351412Z",
          "shell.execute_reply.started": "2022-09-18T02:17:34.341867Z",
          "shell.execute_reply": "2022-09-18T02:17:34.350106Z"
        },
        "trusted": true,
        "id": "JLsd7Wd0BWva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model After **25 Epochs** $:$\n",
        "\n",
        "![image.png](attachment:fab4d007-20b8-4c37-93a9-f66f2524d685.png)"
      ],
      "metadata": {
        "id": "eljtqq74BWvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "4_lpmA7CBWvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HtoZ_gen = load_model(\"../input/imagetoimage-translation-cyclegan/GeneratorHtoZ.h5\")\n",
        "ZtoH_gen = load_model(\"../input/imagetoimage-translation-cyclegan/GeneratorZtoH.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:34.352917Z",
          "iopub.execute_input": "2022-09-18T02:17:34.353912Z",
          "iopub.status.idle": "2022-09-18T02:17:40.138826Z",
          "shell.execute_reply.started": "2022-09-18T02:17:34.353876Z",
          "shell.execute_reply": "2022-09-18T02:17:40.13713Z"
        },
        "trusted": true,
        "id": "3_WXGQC0BWvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_preds(HtoZ_gen, ZtoH_gen, n_images=5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:17:40.140697Z",
          "iopub.execute_input": "2022-09-18T02:17:40.141369Z",
          "iopub.status.idle": "2022-09-18T02:18:15.308317Z",
          "shell.execute_reply.started": "2022-09-18T02:17:40.141318Z",
          "shell.execute_reply": "2022-09-18T02:18:15.307125Z"
        },
        "trusted": true,
        "id": "otoVQHASBWvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HtoZ_gen_25 = load_model(\"../input/imagetoimage-translation-cyclegan/GeneratorHtoZ_25.h5\")\n",
        "ZtoH_gen_25 = load_model(\"../input/imagetoimage-translation-cyclegan/GeneratorZtoH_25.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:18:15.31021Z",
          "iopub.execute_input": "2022-09-18T02:18:15.310903Z",
          "iopub.status.idle": "2022-09-18T02:18:21.846757Z",
          "shell.execute_reply.started": "2022-09-18T02:18:15.310859Z",
          "shell.execute_reply": "2022-09-18T02:18:21.845472Z"
        },
        "trusted": true,
        "id": "1iy1UgbeBWvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_preds(HtoZ_gen_25, ZtoH_gen_25, n_images=5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-18T02:18:21.848649Z",
          "iopub.execute_input": "2022-09-18T02:18:21.849151Z",
          "iopub.status.idle": "2022-09-18T02:18:55.964476Z",
          "shell.execute_reply.started": "2022-09-18T02:18:21.849104Z",
          "shell.execute_reply": "2022-09-18T02:18:55.963259Z"
        },
        "trusted": true,
        "id": "6G96lM8RBWvb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}